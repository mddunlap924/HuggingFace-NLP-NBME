{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" USER INPUTS \"\"\"\n",
    "MODEL_GROUPS = ['17y1th7h'] # Models to use for inference\n",
    "LOCATION = 'Local'  # Local vs. Kaggle Run Directories\n",
    "# https://arthought.com/transformer-model-fine-tuning-for-text-classification-with-pytorch-lightning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "\n",
    "if LOCATION == 'Local':\n",
    "    transformer_path = Path('./venv/lib/python3.9/site-packages/transformers')\n",
    "    input_dir = Path('./input')\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "    from nlp.datasets import TestDataset\n",
    "elif LOCATION == 'Kaggle':\n",
    "    transformer_path = Path('/opt/conda/lib/python3.7/site-packages/transformers')\n",
    "    input_dir = Path('../input')\n",
    "    %env TOKENIZERS_PARALLELISM=true\n",
    "    from nlp_datasets import TestDataset\n",
    "\n",
    "PATHS = {'transformer': transformer_path,\n",
    "         'input': input_dir }\n",
    "print(PATHS)\n",
    "\n",
    "CSV_DATA_PATH = os.path.join(PATHS['input'], 'nbme-score-clinical-patient-notes')\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n",
    "# This must be done before importing transformers\n",
    "import shutil\n",
    "\n",
    "transformers_path = PATHS['transformer']\n",
    "input_dir = PATHS['input'] / 'deberta-v2-3-fast-tokenizer'\n",
    "convert_file = input_dir / \"convert_slow_tokenizer.py\"\n",
    "conversion_path = transformers_path / convert_file.name\n",
    "if conversion_path.exists():\n",
    "    conversion_path.unlink()\n",
    "shutil.copy(convert_file, transformers_path)\n",
    "deberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n",
    "for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n",
    "    filepath = deberta_v2_path / filename\n",
    "    if filepath.exists():\n",
    "        filepath.unlink()\n",
    "    shutil.copy(input_dir / filename, filepath)\n",
    "from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T22:45:28.978481Z",
     "iopub.status.busy": "2022-03-17T22:45:28.978084Z",
     "iopub.status.idle": "2022-03-17T22:45:36.068929Z",
     "shell.execute_reply": "2022-03-17T22:45:36.068106Z",
     "shell.execute_reply.started": "2022-03-17T22:45:28.978446Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    AutoModel,\n",
    ")\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.metrics import f1_score\n",
    "import ast\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on binary arrays.\n",
    "    Args:\n",
    "        preds (list of lists of ints): Predictions.\n",
    "        truths (list of lists of ints): Ground truths.\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    # Micro : aggregating over all instances\n",
    "    preds = np.concatenate(preds)\n",
    "    truths = np.concatenate(truths)\n",
    "    return f1_score(truths, preds)\n",
    "\n",
    "\n",
    "def spans_to_binary(spans, length=None):\n",
    "    \"\"\"\n",
    "    Converts spans to a binary array indicating whether each character is in the span.\n",
    "    Args:\n",
    "        spans (list of lists of two ints): Spans.\n",
    "    Returns:\n",
    "        np array [length]: Binarized spans.\n",
    "    \"\"\"\n",
    "    length = np.max(spans) if length is None else length\n",
    "    binary = np.zeros(length)\n",
    "    for start, end in spans:\n",
    "        binary[start:end] = 1\n",
    "\n",
    "    return binary\n",
    "\n",
    "\n",
    "def span_micro_f1(preds, truths):\n",
    "    \"\"\"\n",
    "    Micro f1 on spans.\n",
    "    Args:\n",
    "        preds (list of lists of two ints): Prediction spans.\n",
    "        truths (list of lists of two ints): Ground truth spans.\n",
    "    Returns:\n",
    "        float: f1 score.\n",
    "    \"\"\"\n",
    "    bin_preds = []\n",
    "    bin_truths = []\n",
    "    for pred, truth in zip(preds, truths):\n",
    "        if not len(pred) and not len(truth):\n",
    "            continue\n",
    "        length = max(np.max(pred) if len(pred) else 0, np.max(truth) if len(truth) else 0)\n",
    "        bin_preds.append(spans_to_binary(pred, length))\n",
    "        bin_truths.append(spans_to_binary(truth, length))\n",
    "    return micro_f1(bin_preds, bin_truths)\n",
    "\n",
    "\n",
    "def create_labels_for_scoring(df):\n",
    "    # example: ['0 1', '3 4'] -> ['0 1; 3 4']\n",
    "    df['location_for_create_labels'] = [ast.literal_eval(f'[]')] * len(df)\n",
    "    for i in range(len(df)):\n",
    "        lst = df.loc[i, 'location']\n",
    "        if lst:\n",
    "            new_lst = ';'.join(lst)\n",
    "            df.loc[i, 'location_for_create_labels'] = ast.literal_eval(f'[[\"{new_lst}\"]]')\n",
    "    # create labels\n",
    "    truths = []\n",
    "    for location_list in df['location_for_create_labels'].values:\n",
    "        truth = []\n",
    "        if len(location_list) > 0:\n",
    "            location = location_list[0]\n",
    "            for loc in [s.split() for s in location.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                truth.append([start, end])\n",
    "        truths.append(truth)\n",
    "    return truths\n",
    "\n",
    "\n",
    "def get_char_probs(texts, predictions, tokenizer):\n",
    "    results = [np.zeros(len(t)) for t in texts]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n",
    "        encoded = tokenizer(text,\n",
    "                            add_special_tokens=True,\n",
    "                            return_offsets_mapping=True)\n",
    "        for idx, (offset_mapping, pred) in enumerate(zip(encoded['offset_mapping'], prediction)):\n",
    "            start = offset_mapping[0]\n",
    "            end = offset_mapping[1]\n",
    "            results[i][start:end] = pred\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_results(char_probs, th=0.5):\n",
    "    results = []\n",
    "    for char_prob in char_probs:\n",
    "        result = np.where(char_prob >= th)[0] + 1\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_predictions(results):\n",
    "    predictions = []\n",
    "    for result in results:\n",
    "        prediction = []\n",
    "        if result != \"\":\n",
    "            for loc in [s.split() for s in result.split(';')]:\n",
    "                start, end = int(loc[0]), int(loc[1])\n",
    "                prediction.append([start, end])\n",
    "        predictions.append(prediction)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def get_score(y_true, y_pred):\n",
    "    score = span_micro_f1(y_true, y_pred)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def dict2obj(d):\n",
    "    # checking whether object d is a\n",
    "    # instance of class list\n",
    "    if isinstance(d, list):\n",
    "        d = [dict2obj(x) for x in d]\n",
    "\n",
    "        # if d is not a instance of dict then\n",
    "    # directly object is returned\n",
    "    if not isinstance(d, dict):\n",
    "        return d\n",
    "\n",
    "    # declaring a class\n",
    "    class C:\n",
    "        pass\n",
    "\n",
    "    # constructor of the class passed to obj\n",
    "    obj = C()\n",
    "\n",
    "    for k in d:\n",
    "        obj.__dict__[k] = dict2obj(d[k])\n",
    "\n",
    "    return obj\n",
    "\n",
    "\n",
    "def get_char_probs(texts, predictions, tokenizer):\n",
    "    \"\"\" Probability for each Character \"\"\"\n",
    "    results = [np.zeros(len(t)) for t in texts]\n",
    "    for i, (text, prediction) in enumerate(zip(texts, predictions)):\n",
    "        encoded = tokenizer(text,\n",
    "                            add_special_tokens=True,\n",
    "                            return_offsets_mapping=True)\n",
    "        for idx, (offset_mapping, pred) in enumerate(zip(encoded['offset_mapping'], prediction)):\n",
    "            start = offset_mapping[0]\n",
    "            end = offset_mapping[1]\n",
    "            results[i][start:end] = pred\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_results(char_probs, th=0.5):\n",
    "    results = []\n",
    "    for char_prob in char_probs:\n",
    "        result = np.where(char_prob >= th)[0] + 1\n",
    "        result = [list(g) for _, g in itertools.groupby(result, key=lambda n, c=itertools.count(): n - next(c))]\n",
    "        result = [f\"{min(r)} {max(r)}\" for r in result]\n",
    "        result = \";\".join(result)\n",
    "        results.append(result)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T22:45:36.146418Z",
     "iopub.status.busy": "2022-03-17T22:45:36.145762Z",
     "iopub.status.idle": "2022-03-17T22:45:36.165587Z",
     "shell.execute_reply": "2022-03-17T22:45:36.164748Z",
     "shell.execute_reply.started": "2022-03-17T22:45:36.146380Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def list_model_folds(group_id):\n",
    "    \"\"\" List all models and their files (i.e., *.ckpt and *.config) \"\"\"\n",
    "    list_models = []\n",
    "    filelist = []\n",
    "    model_group = os.path.join(PATHS['input'], 'nbme-' + group_id)\n",
    "\n",
    "    for root, dirs, files in os.walk(model_group):\n",
    "        for file in files:\n",
    "            # append the file name to the list\n",
    "            filelist.append(os.path.join(root, file))\n",
    "\n",
    "    model_folds = []\n",
    "    for filelist_ in filelist:\n",
    "        # fold_id = filelist_.split(model_group)[-1].split(f'/{group_id}/')[-1].split('/')[1].split('_')[-1]\n",
    "        fold_id = filelist_.split('_')[-1].split('/')[0]\n",
    "        if fold_id not in model_folds:\n",
    "            model_folds.append(fold_id)\n",
    "\n",
    "    config, ckpt = None, None\n",
    "    for model_fold in model_folds:\n",
    "        for filelist_ in filelist:\n",
    "            if (model_fold in filelist_) and ('.yaml' in filelist_):\n",
    "                config = filelist_\n",
    "            if (model_fold in filelist_) and ('.ckpt' in filelist_):\n",
    "                ckpt = filelist_\n",
    "        list_models.append([config, ckpt])\n",
    "    return list_models\n",
    "\n",
    "\"\"\" List all models and files for prediction \"\"\"\n",
    "all_model_files = []\n",
    "for MODEL_GROUP in MODEL_GROUPS:\n",
    "    model_folds = list_model_folds(group_id=MODEL_GROUP)\n",
    "    for model_fold in model_folds:\n",
    "        all_model_files.append(model_fold)\n",
    "\n",
    "print(all_model_files)\n",
    "for all_model_file in all_model_files:\n",
    "    print(f'Model Files: {all_model_file}')\n",
    "\n",
    "print('check here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T22:45:36.168449Z",
     "iopub.status.busy": "2022-03-17T22:45:36.167989Z",
     "iopub.status.idle": "2022-03-17T22:45:36.191053Z",
     "shell.execute_reply": "2022-03-17T22:45:36.190245Z",
     "shell.execute_reply.started": "2022-03-17T22:45:36.168410Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class BaseLineModel(pl.LightningModule):\n",
    "    def __init__(self, model_repo, *,\n",
    "                 autoconfig_path=None,\n",
    "                 automodel_path=None,\n",
    "                 learning_rate: float = 2e-5,\n",
    "                 adam_epsilon: float = 1e-8,\n",
    "                 warmup_steps: int = 0,\n",
    "                 weight_decay: float = 0.0,\n",
    "                 dropout_rate=0.1,\n",
    "                 th: float = 0.5,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        print(f'Save Hyperparameters: {self.save_hyperparameters()}')\n",
    "        self.th = th\n",
    "        if autoconfig_path is None:\n",
    "            autoconfig_path = PATHS['input'] / self.hparams.model_repo / 'config.json'\n",
    "        if automodel_path is None:\n",
    "            automodel_path = PATHS['input'] / self.hparams.model_repo\n",
    "\n",
    "        print(f'Loading AutoConfig inside Model Class: {autoconfig_path}')\n",
    "        self.config = AutoConfig.from_pretrained(autoconfig_path, output_hidden_states=True)\n",
    "        print('\\tLoaded AutoConfig inside Model Class')\n",
    "        print(f'Loading AutoModel inside Model Class: {automodel_path}')\n",
    "        self.base = AutoModel.from_pretrained(automodel_path, config=self.config)\n",
    "        print('\\tLoaded AutoModel inside Model Class')\n",
    "        self.fc_dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 1)\n",
    "\n",
    "    def feature(self, inputs):\n",
    "        outputs = self.base(**inputs)\n",
    "        last_hidden_states = outputs[0]\n",
    "        return last_hidden_states\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        outs = self.fc(self.fc_dropout(feature))\n",
    "        return outs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels, idx = batch\n",
    "        y_preds = self(inputs)\n",
    "        loss = nn.BCEWithLogitsLoss(reduction=\"none\")(y_preds.view(-1, 1), labels.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, labels.view(-1, 1) != -1).mean()\n",
    "        # return {\"loss\": loss, \"preds\": y_preds.detach(), \"labels\": labels.detach(), \"idxs\": idx.detach()}\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        inputs, labels, idx = batch\n",
    "        preds = self(inputs)\n",
    "        val_loss = nn.BCEWithLogitsLoss(reduction=\"none\")(preds.view(-1, 1), labels.view(-1, 1))\n",
    "        val_loss = torch.masked_select(val_loss, labels.view(-1, 1) != -1).mean()\n",
    "        self.log(\"val_loss\", val_loss, prog_bar=True)\n",
    "        return {\"loss\": val_loss, \"preds\": preds, \"labels\": labels, \"idxs\": idx}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Prepare optimizer and schedule (linear warmup and decay)\"\"\"\n",
    "        model = self\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.hparams.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters,\n",
    "                          lr=self.hparams.learning_rate,\n",
    "                          eps=self.hparams.adam_epsilon)\n",
    "        return [optimizer]\n",
    "\n",
    "    def training_epoch_end(self, train_step_outputs):\n",
    "        if not self.trainer.sanity_checking:\n",
    "            # Average Training Loss\n",
    "            train_avg_loss = float(torch.tensor([x[\"loss\"] for x in train_step_outputs]).mean().cpu().numpy())\n",
    "            self.log('train_avg_loss', train_avg_loss, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def validation_epoch_end(self, val_step_outputs):\n",
    "        if not self.trainer.sanity_checking:\n",
    "            # Scoring\n",
    "            idxs = torch.cat([x[\"idxs\"] for x in val_step_outputs]).detach().cpu().numpy()\n",
    "            val_texts = [self.trainer.datamodule.val_texts[idx] for idx in idxs]\n",
    "            val_labels = [self.trainer.datamodule.val_labels[idx] for idx in idxs]\n",
    "            predictions = np.squeeze(torch.cat([x[\"preds\"].sigmoid() for x in val_step_outputs]).detach().cpu().numpy())\n",
    "            char_probs = get_char_probs(val_texts, predictions, self.trainer.datamodule.cfg.tokenizer)\n",
    "            results = get_results(char_probs, th=self.th)\n",
    "            preds = get_predictions(results)\n",
    "            score = get_score(val_labels, preds)\n",
    "            self.log('val_avg_f1', score, on_epoch=True, prog_bar=True)\n",
    "\n",
    "            # Average Validation Loss\n",
    "            val_avg_loss = float(torch.tensor([x[\"loss\"] for x in val_step_outputs]).mean().cpu().numpy())\n",
    "            self.log('val_avg_loss', val_avg_loss, on_epoch=True, prog_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def inference_fn(test_loader, model, device):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "\n",
    "    tk0 = tqdm(test_loader, total=len(test_loader))\n",
    "    for inputs in tk0:\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        preds.append(y_preds.sigmoid().to('cpu').numpy())\n",
    "    predictions_ = np.concatenate(preds)\n",
    "    return predictions_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-17T22:45:36.194770Z",
     "iopub.status.busy": "2022-03-17T22:45:36.194208Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Data Loading \"\"\"\n",
    "test = pd.read_csv(os.path.join(CSV_DATA_PATH, 'test.csv'))\n",
    "submission = pd.read_csv(os.path.join(CSV_DATA_PATH, 'sample_submission.csv'))\n",
    "features = pd.read_csv(os.path.join(CSV_DATA_PATH, 'features.csv'))\n",
    "patient_notes = pd.read_csv(os.path.join(CSV_DATA_PATH, 'patient_notes.csv'))\n",
    "test = test.merge(features, on=['feature_num', 'case_num'], how='left')\n",
    "test = test.merge(patient_notes, on=['pn_num', 'case_num'], how='left')\n",
    "\n",
    "\"\"\" Loop through each model and make inference \"\"\"\n",
    "predictions = []\n",
    "for model_files in all_model_files:\n",
    "    config_path, ckpt_path = model_files[0], model_files[1]\n",
    "\n",
    "    \"\"\" Config File \"\"\"\n",
    "    CFG = MODEL_GROUP\n",
    "    with open(config_path, 'r') as file:\n",
    "        CFG = yaml.safe_load(file)\n",
    "    file.close()\n",
    "    CFG = dict2obj(CFG)\n",
    "\n",
    "    \"\"\" Tokenizer \"\"\"\n",
    "    print(f'About to load tokenizer: {CFG.model}')\n",
    "    if \"large\" in CFG.model:\n",
    "        CFG.tokenizer = DebertaV2TokenizerFast.from_pretrained(PATHS['input'] / 'get-token/tokenizer')\n",
    "    else:\n",
    "        CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.model_repo)\n",
    "    print('LOADED TOKENIZER')\n",
    "\n",
    "    \"\"\" Test Dataset \"\"\"\n",
    "    test_dataset = TestDataset(CFG, test)\n",
    "    test_loader = DataLoader(test_dataset,\n",
    "                             batch_size=CFG.batch_size,\n",
    "                             shuffle=False,\n",
    "                             pin_memory=True,\n",
    "                             drop_last=False,\n",
    "                             )\n",
    "    print(\"LOADED TEST DATASETS\")\n",
    "\n",
    "    \"\"\" Load Inference Model \"\"\"\n",
    "    autoconfig_path = PATHS['input'] / CFG.model\n",
    "    automodel_path = ckpt_path\n",
    "    print(f'LOAD MODEL: {ckpt_path}\\n'\n",
    "          f'autoconfig_path: {autoconfig_path}\\n'\n",
    "          f'automodel_path: {automodel_path}\\n')\n",
    "    nbme_model = BaseLineModel.load_from_checkpoint(ckpt_path)\n",
    "    # nbme_model = BaseLineModel(model_repo=CFG.model,\n",
    "    #                            autoconfig_path=autoconfig_path,\n",
    "    #                            automodel_path=automodel_path,\n",
    "    #                            dropout_rate=CFG.fc_dropout).load_from_checkpoint(ckpt_path)\n",
    "    # nbme_model = BaseLineModel(model_repo=CFG.model)\n",
    "    nbme_model = BaseLineModel.load_from_checkpoint(ckpt_path, autoconfig_path=autoconfig_path, automodel_path=automodel_path)\n",
    "    nbme_model.to(DEVICE)\n",
    "    nbme_model.freeze()\n",
    "    nbme_model.eval()\n",
    "    print(f'LOADED BASE MODEL: {ckpt_path}')\n",
    "    prediction = inference_fn(test_loader, nbme_model, DEVICE)\n",
    "    prediction = prediction.reshape((len(test), CFG.max_len))\n",
    "    char_probs = get_char_probs(test['pn_history'].values, prediction, CFG.tokenizer)\n",
    "    predictions.append(char_probs)\n",
    "    del nbme_model, prediction, char_probs\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f'Complted Inference: {ckpt_path}\\n')\n",
    "\n",
    "predictions = np.mean(predictions, axis=0)\n",
    "\n",
    "\"\"\" Submission \"\"\"\n",
    "print('Started Submission Section')\n",
    "results = get_results(predictions, th=0.1)\n",
    "submission['location'] = results\n",
    "print(submission.head())\n",
    "submission[['id', 'location']].to_csv('submission.csv', index=False)\n",
    "print('Completed Submission')\n",
    "print('End of Inference')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
